{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def leer_archivos(carpeta):\n",
    "    documentos = {}\n",
    "\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        match = re.search(r\"\\d+\", archivo)  # Extraer el numero\n",
    "        doc_id = int(match.group())\n",
    "        with open(os.path.join(carpeta, archivo), \"r\", encoding=\"utf-8\") as f:\n",
    "            documentos[doc_id] = f.read()\n",
    "\n",
    "    return documentos\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    tokens_procesados = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens_procesados\n",
    "\n",
    "def sacar_acentos(texto):\n",
    "    texto = re.sub(r\"[áàäâã]\", \"a\", texto)\n",
    "    texto = re.sub(r\"[éèëê]\", \"e\", texto)\n",
    "    texto = re.sub(r\"[íìïî]\", \"i\", texto)\n",
    "    texto = re.sub(r\"[óòöôõ]\", \"o\", texto)\n",
    "    texto = re.sub(r\"[úùüû]\", \"u\", texto)\n",
    "    return texto\n",
    "\n",
    "def procesar(texto, doc_id):\n",
    "    texto = texto.lower()  # Convertir a minusculas\n",
    "    texto = sacar_acentos(texto)  # Eliminar acentos\n",
    "    texto = re.sub(r\"[^a-z\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
    "    tokens = texto.split()  # Tokenizar (En este caso separo por espacios)\n",
    "    tokens = stemming(tokens)\n",
    "\n",
    "    global nTokens\n",
    "    global datos\n",
    "\n",
    "    nTokens += len(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in datos:\n",
    "            datos[token] = {\n",
    "                \"docsId\": [],\n",
    "                \"df\": 0,\n",
    "            }  # Inicializo la estructura de cada termino\n",
    "\n",
    "        if doc_id not in datos[token][\"docsId\"]:\n",
    "            datos[token][\"docsId\"].append(doc_id)\n",
    "            datos[token][\"df\"] = len(datos[token][\"docsId\"])\n",
    "\n",
    "\n",
    "def analisis_lexico(carpeta):\n",
    "    global nDocumentos\n",
    "    documentos = leer_archivos(carpeta)\n",
    "    for doc_id, contenido in documentos.items():\n",
    "        procesar(contenido, doc_id)\n",
    "        nDocumentos += 1\n",
    "\n",
    "\n",
    "datos = {}\n",
    "nTokens = 0\n",
    "nDocumentos = 0\n",
    "path = \"collection_test/TestCollection\"\n",
    "\n",
    "analisis_lexico(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lista de términos y su DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gat: 400\n",
      "lor: 500\n",
      "telescopi: 1500\n",
      "botell: 2000\n",
      "automovil: 4000\n",
      "alfombr: 10000\n",
      "calefactor: 1200\n",
      "lluvi: 3200\n",
      "manual: 1300\n",
      "ventanal: 1100\n",
      "comput: 2200\n",
      "reloj: 700\n",
      "llav: 800\n",
      "puert: 900\n",
      "tenis: 1300\n",
      "pelot: 1400\n",
      "mes: 600\n",
      "perr: 300\n",
      "sillon: 1000\n",
      "cas: 200\n"
     ]
    }
   ],
   "source": [
    "for termino, info in datos.items():\n",
    "    print(f\"{termino}: {info['df']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793254\n"
     ]
    }
   ],
   "source": [
    "print(nTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de términos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "nTerminos = len(list(datos.keys()))\n",
    "print(nTerminos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de documentos procesados  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(nDocumentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'casa'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m termino = term[\u001b[33m'\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m df = term[\u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdatos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtermino\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m] != df:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFallo en el test - \u001b[39m\u001b[33m\"\u001b[39m + termino)\n\u001b[32m     10\u001b[39m     exit\n",
      "\u001b[31mKeyError\u001b[39m: 'casa'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"collection_test/collection_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "    for term in test_data['data']:\n",
    "        termino = term['term']\n",
    "        df = term['df']\n",
    "        if datos[termino]['df'] != df:\n",
    "            print(\"Fallo en el test - \" + termino)\n",
    "            exit\n",
    "        \n",
    "    if (nTokens != test_data['statistics']['num_tokens']):\n",
    "        print(\"Fallo en el test nTokens\")\n",
    "        exit\n",
    "        \n",
    "    if (nDocumentos != test_data['statistics']['N']):\n",
    "        print(\"Fallo en el test nDocumentos\")\n",
    "        exit\n",
    "        \n",
    "    if (nTerminos != test_data['statistics']['num_terms']):\n",
    "        print(\"Fallo en el test nTerminos\")\n",
    "        exit\n",
    "        \n",
    "    print(\"Test exitoso!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
