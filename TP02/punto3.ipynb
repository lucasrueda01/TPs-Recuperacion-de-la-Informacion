{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escriba un segundo tokenizer que implemente los criterios del artículo de Grefenstette y Tapanainen para definir qué es una “palabra” (o término) y cómo tratar números y signos de puntuación. En este  caso  su  tokenizer deberá extraer y tratar como un único término: \n",
    "\n",
    "Abreviaturas tal cual están escritas (por ejemplo, Dr., Lic., S.A., etc.)\n",
    "Direcciones de correo electrónico y URLs.\n",
    "Números (por ejemplo, cantidades, teléfonos).\n",
    "Nombres propios (por ejemplo, Villa Carlos Paz, Manuel Belgrano, etc.)\n",
    " \n",
    "Utilice la colección para debugging de expresiones regulares provista por el equipo docente para extraer y comparar  la salida de su programa con los metadatos de la colección tal como lo realizó en el punto 1.\n",
    "\n",
    "Por último, extraiga y almacene la misma información que en el punto 2 sobre la colección RI-tknz-data utilizando su nuevo tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def leer_archivos(carpeta):\n",
    "    documentos = {}\n",
    "\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        match = re.search(r\"\\d+\", archivo)  # Extraer el numero\n",
    "        doc_id = int(match.group())\n",
    "        with open(os.path.join(carpeta, archivo), \"r\", encoding=\"utf-8\") as f:\n",
    "            documentos[doc_id] = f.read()\n",
    "\n",
    "    return documentos\n",
    "\n",
    "def tokenizer(texto):\n",
    "    # Extraer terminos especiales\n",
    "    urls = re.findall(r\"(?:https?|ftp)://[^\\s]+\", texto)\n",
    "    emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", texto) # usuario@dominio.com\n",
    "    numeros = re.findall(r\"\\(?\\d{2,4}\\)?[-.\\s]?\\d{2,4}[-.\\s]?\\d{2,4}\", texto) # (012) 345-6789\n",
    "    nombres_propios = re.findall(r\"[A-ZÁÉÍÓÚÑ][a-záéíóúñ]+(?:\\s[A-ZÁÉÍÓÚÑ][a-záéíóúñ]+)+\", texto) # Juan Pérez\n",
    "    acronimos = re.findall(r\"\\b(?:[A-Z]+\\.){2,}\", texto) # NO FUNCIONA \n",
    "    abreviaturas = re.findall(r\"\\b(?:[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{1,4}\\.)\", texto) # NO FUNCIONA\n",
    "\n",
    "\n",
    "    # Eliminar esos términos del texto para no procesarlos dos veces\n",
    "    \n",
    "    for termino in urls + emails + numeros + nombres_propios + abreviaturas + acronimos:\n",
    "        texto = texto.replace(termino, \"\")\n",
    "\n",
    "    # Eliminar caracteres especiales\n",
    "    texto = re.sub(r\"[^a-zA-ZÁÉÍÓÚÑáéíóúñ0-9\\s]\", \"\", texto)\n",
    "    # Convertir a minusculas\n",
    "    texto = texto.lower()\n",
    "    # Tokenizar (separar por espacios)\n",
    "    tokens = texto.split()\n",
    "\n",
    "    # Restaurar los términos especiales en la lista final de tokens\n",
    "    tokens.extend(urls + emails + numeros + nombres_propios + abreviaturas + acronimos)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def procesar(texto, doc_id):\n",
    "\n",
    "    global nTokens\n",
    "    global datos\n",
    "\n",
    "    tokens = tokenizer(texto)\n",
    "    nTokens += len(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in datos:\n",
    "            datos[token] = {\n",
    "                \"docsId\": [],\n",
    "                \"df\": 0,\n",
    "            }  # Inicializo la estructura de cada termino\n",
    "\n",
    "        if doc_id not in datos[token][\"docsId\"]:\n",
    "            datos[token][\"docsId\"].append(doc_id)\n",
    "            datos[token][\"df\"] = len(datos[token][\"docsId\"])\n",
    "\n",
    "\n",
    "def analisis_lexico(carpeta):\n",
    "    global nDocumentos\n",
    "    documentos = leer_archivos(carpeta)\n",
    "    for doc_id, contenido in documentos.items():\n",
    "        procesar(contenido, doc_id)\n",
    "        nDocumentos += 1\n",
    "\n",
    "\n",
    "datos = {}\n",
    "nTokens = 0\n",
    "nDocumentos = 0\n",
    "path = \"RE_collection_test/collection_test_ER2\"\n",
    "\n",
    "analisis_lexico(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lista de términos y su DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lluvia: 320\n",
      "tenis: 130\n",
      "https://www.youtube.com/watch?v=zKqUfwB84Zg: 60\n",
      "https://www.researchgate.net/publication/2360239: 1000\n",
      "2021-12-09: 132\n",
      "2360239: 998\n",
      "Domingo Faustino Sarmiento: 990\n",
      "https://docs.google.com/document/d/1ninD55Cfbb_7PksDirN0XghzNHJZ_N93lheQzF1aOZY/edit?usp=sharing: 120\n",
      "abc.def@mail-archive.com: 90\n",
      "15413355: 220\n",
      "2360239 1541: 215\n",
      "3355 15413355: 197\n",
      "Sra.: 80\n",
      "ternero: 400\n",
      "3355: 46\n",
      "15413355 1541: 166\n",
      "http://www.tcpipguide.com/free/t_TCPIPProcessesMultiplexingandClientServerApplicati.htm: 110\n",
      "i: 150\n",
      "D.A.S.M.: 150\n",
      "EE.UU.: 70\n",
      "20211209: 23\n",
      "botella: 200\n",
      "2360239 2021: 136\n",
      "12-09 2021: 96\n",
      "mratto@mail.unlu.edu.ar: 100\n",
      "https://www.labredes.unlu.edu.ar/tyr2022: 50\n",
      "2022 15413355: 10\n",
      "a: 30\n",
      "U.S.: 30\n",
      "domingo: 9\n",
      "faustino: 9\n",
      "sarmiento: 9\n",
      "Zg Domingo Faustino Sarmiento: 9\n",
      "ftp://unlu.edu.ar: 40\n",
      "1541: 45\n",
      "casa: 20\n",
      "2022 2021-12: 5\n",
      "09: 1\n",
      "154109: 2\n",
      "3355 2021-12: 20\n",
      "09 2021-12: 8\n",
      "09 15413355: 9\n",
      "12-09 1541: 19\n",
      "15413355 2021: 20\n",
      "Domingo Faustino Sarmiento Sra: 1\n",
      "1209: 2\n",
      "20213355: 2\n",
      "202109: 1\n"
     ]
    }
   ],
   "source": [
    "for termino, info in datos.items():\n",
    "    print(f\"{termino}: {info['df']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111688\n"
     ]
    }
   ],
   "source": [
    "print(nTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de términos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "nTerminos = len(list(datos.keys()))\n",
    "print(nTerminos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de documentos procesados  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(nDocumentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casa : 20\n",
      "U.S.A : 30\n",
      "ftp://unlu.edu.ar : 40\n",
      "https://www.labredes.unlu.edu.ar/tyr2022 : 50\n",
      "https://www.youtube.com/watch?v=zKqUfwB84Zg : 60\n",
      "EE.UU. : 70\n",
      "Sra. : 80\n",
      "abc.def@mail-archive.com : 90\n",
      "mratto@mail.unlu.edu.ar : 100\n",
      "http://www.tcpipguide.com/free/t_TCPIPProcessesMultiplexingandClientServerApplicati.htm : 110\n",
      "https://docs.google.com/document/d/1ninD55Cfbb_7PksDirN0XghzNHJZ_N93lheQzF1aOZY/edit?usp=sharing : 120\n",
      "tenis : 130\n",
      "2021-12-09 : 140\n",
      "D.A.S.M.I : 150\n",
      "botella : 200\n",
      "ternero : 400\n",
      "15413355 : 220\n",
      "Domingo Faustino Sarmiento : 1000\n",
      "lluvia : 320\n",
      "https://www.researchgate.net/publication/2360239 : 1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"RE_collection_test/collection_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "    for term in test_data['data']:\n",
    "        print(term['term'] + \" : \" + str(term['df']))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
