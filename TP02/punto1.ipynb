{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la colección para debugging provista por el equipo docente escriba un programa que realice el análisis léxico sobre la misma y extraiga la siguiente información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def leer_archivos(carpeta):\n",
    "    documentos = {}\n",
    "\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        match = re.search(r\"\\d+\", archivo)  # Extraer el numero\n",
    "        doc_id = int(match.group())\n",
    "        with open(os.path.join(carpeta, archivo), \"r\", encoding=\"utf-8\") as f:\n",
    "            documentos[doc_id] = f.read()\n",
    "\n",
    "    return documentos\n",
    "\n",
    "\n",
    "def procesar(texto, doc_id):\n",
    "    texto = texto.lower()  # Convertir a minusculas\n",
    "    texto = re.sub(r\"[^a-z\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
    "    tokens = texto.split()  # Tokenizar (En este caso separo por espacios)\n",
    "\n",
    "    global nTokens\n",
    "    global datos\n",
    "\n",
    "    nTokens += len(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in datos:\n",
    "            datos[token] = {\n",
    "                \"docsId\": [],\n",
    "                \"df\": 0,\n",
    "            }  # Inicializo la estructura de cada termino\n",
    "\n",
    "        if doc_id not in datos[token][\"docsId\"]:\n",
    "            datos[token][\"docsId\"].append(doc_id)\n",
    "            datos[token][\"df\"] = len(datos[token][\"docsId\"])\n",
    "\n",
    "\n",
    "def analisis_lexico(carpeta):\n",
    "    global nDocumentos\n",
    "    documentos = leer_archivos(carpeta)\n",
    "    for doc_id, contenido in documentos.items():\n",
    "        procesar(contenido, doc_id)\n",
    "        nDocumentos += 1\n",
    "\n",
    "\n",
    "datos = {}\n",
    "nTokens = 0\n",
    "nDocumentos = 0\n",
    "path = \"collection_test/TestCollection\"\n",
    "\n",
    "analisis_lexico(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lista de términos y su DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato: 400\n",
      "loro: 500\n",
      "telescopio: 1500\n",
      "botella: 2000\n",
      "automovil: 4000\n",
      "alfombra: 10000\n",
      "calefactores: 1200\n",
      "lluvia: 3200\n",
      "manualidades: 1300\n",
      "ventanal: 1100\n",
      "computadora: 2200\n",
      "reloj: 700\n",
      "llave: 800\n",
      "puerta: 900\n",
      "tenis: 1300\n",
      "pelotas: 1400\n",
      "mesa: 600\n",
      "perro: 300\n",
      "sillon: 1000\n",
      "casa: 200\n"
     ]
    }
   ],
   "source": [
    "for termino, info in datos.items():\n",
    "    print(f\"{termino}: {info['df']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793254\n"
     ]
    }
   ],
   "source": [
    "print(nTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de términos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "nTerminos = len(list(datos.keys()))\n",
    "print(nTerminos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cantidad de documentos procesados  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(nDocumentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test exitoso!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"collection_test/collection_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "    for term in test_data['data']:\n",
    "        termino = term['term']\n",
    "        df = term['df']\n",
    "        if datos[termino]['df'] != df:\n",
    "            print(\"Fallo en el test - \" + termino)\n",
    "            exit\n",
    "        \n",
    "    if (nTokens != test_data['statistics']['num_tokens']):\n",
    "        print(\"Fallo en el test nTokens\")\n",
    "        exit\n",
    "        \n",
    "    if (nDocumentos != test_data['statistics']['N']):\n",
    "        print(\"Fallo en el test nDocumentos\")\n",
    "        exit\n",
    "        \n",
    "    if (nTerminos != test_data['statistics']['num_terms']):\n",
    "        print(\"Fallo en el test nTerminos\")\n",
    "        exit\n",
    "        \n",
    "    print(\"Test exitoso!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
